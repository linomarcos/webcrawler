"""
Search query infrastructure for indexed web pages.

The method searcher.buildIndex 'transposes the matrix' of urls-vs-words in the database generated by the crawler class. This is the key toward accellerating queries.
"""

import pymongo
import crawler, wordcount
from pysqlite2 import dbapi2 as sqlite

class searcher(wordcount.wordcount):
	"""Main usage of the class is via 2 methods:
	(1) buildIndex() = builds 'transpose matrix' for querying words against urls and locations,
	(2) simpleQuery(str) = for a string 'str' of whitespace-separated tokens, returns a dictionary of all urls containing those tokens plus information about their locations.

	The collection of indexed webpages is accessible via self.indexdb, and is initialized in the parent class. 
	"""
	def __init__(self, dbPrefix='test'):
		# call base class constructor
		wordcount.wordcount.__init__(self, dbPrefix)	
		wordsName = '.'.join([dbPrefix,'sqlite'])
		print "word index: '"+wordsName+"'"
		self.wordsdb = sqlite.connect(wordsName)

	def __del__(self):
		# ignore the destructor of the parent class; close the Mongo connection here.
		self._mdbcon.disconnect()
		self.wordsdb.close()
	
	def initTables(self):
		"""Create the database tables."""
		self.wordsdb.execute('drop table if exists urllist')
		self.wordsdb.execute('drop table if exists wordlist')
		self.wordsdb.execute('drop table if exists wordgraph')
		self.wordsdb.execute('drop index if exists urlidx')
		self.wordsdb.execute('drop index if exists wordidx')
		self.wordsdb.execute('drop index if exists graphidx')
		
		self.wordsdb.execute('create table urllist(url)')
		self.wordsdb.execute('create table wordlist(word)')
		self.wordsdb.execute('create table wordgraph(urlid, wordid, pos)')
		self.wordsdb.execute('create index urlidx on urllist(url)')
		self.wordsdb.execute('create index wordidx on wordlist(word)')
		self.wordsdb.execute('create index graphidx on wordgraph(wordid)')
	
	def getEntryID(self, table, field, value, createnew=True):
		"""Returns rowid from a table; creates a row if value is not present."""		
		cmd='select rowid from %s where %s="%s"' % (table, field, value)
		result = self.wordsdb.execute(cmd).fetchone()
		if not result:
			cmd='insert into %s (%s) values("%s")' % (table, field, value)
			cur = self.wordsdb.execute(cmd)
			return cur.lastrowid
		else:
			return result[0]
	
	def buildIndex(self):
		"""Builds a reduced map of words-to-url_id's for accelerating queries."""
		# iterate over all records in the "index" database, exploiting the
		# fact that JSON objects are converted to Python dictionaries.
		print self.__module__, ': building word index'
		for rec in self.indexdb.find():
			urlid = self.getEntryID('urllist', 'url', rec['url'])
			words = rec['words']
			for j in range(len(words)):
				if words[j] in self.blackList:
					continue
				wordid = self.getEntryID('wordlist', 'word', words[j])
				cmd='insert into wordgraph(urlid, wordid, pos) values (%d, %d, %d)' % (urlid, wordid, j)
				self.wordsdb.execute(cmd)
		self.wordsdb.commit()

	def simpleQuery(self, queryStr):
		"""Tokenizes the query string and finds documents containing all tokens.
		Returns: a pair (tokens, results), where:
		- tokens = list of tokenized search terms.
		- results = dictionary of structured data:
		{ ObjectID(): { token[0]:[..positions..], token[1]:[..positions..],..},...}
		That is, each record is indexed by an ObjectID, representing a document. Its value is a dictionary whose key-value pairs are each a token and a list of positions (integers) of that token in the document.
		"""
		# Example query:
		# SELECT w0.urlid, w0.pos, w1.pos, w2.pos 
		# FROM wordgraph w0, wordgraph w1, wordgraph w2 
		# WHERE w0.wordid = 255
		# and w0.urlid = w1.urlid and w1.wordid = 1192
		# and w1.urlid = w2.urlid and w2.wordid = 73
		
		results = {}
		tokens = crawler.tokenize(queryStr)
		fieldlist = 'w0.urlid'
		tablelist = ''
		clauselist = ''
		tablecount = 0
		
		for word in tokens:
			cmd='select rowid from wordlist where word = "%s"' % word
			wordrow = self.wordsdb.execute(cmd).fetchone()
			if not wordrow:
				return (tokens, results)
			wordid = wordrow[0]
			if tablecount > 0:
				tablelist += ', '
				clauselist += ' and w%d.urlid = w%d.urlid and ' % (tablecount-1, tablecount)
			fieldlist += ', w%d.pos' % tablecount
			tablelist += 'wordgraph w%d' % tablecount
			clauselist += 'w%d.wordid = %d' % (tablecount, wordid)
			tablecount += 1
		
		if tablecount == 0:
			return (tokens, results)
		
		cmd = 'select %s from %s where %s' % (fieldlist, tablelist, clauselist)
		for rec in list(self.wordsdb.execute(cmd)) :
			cmd = 'select url from urllist where rowid=%d' % rec[0]
			urlTpl = self.wordsdb.execute(cmd).fetchone()
			data = results.setdefault(urlTpl[0], {})
			for j in range(len(tokens)) :
				poslist = data.setdefault(tokens[j], [])
				poslist.append(rec[j+1]) # position of resp token in the document

		return (tokens, results)

# include unit tests out of laziness...
import unittest

class testAll(unittest.TestCase):
	"""Unit tests for the searcher class."""
	eta = 0.14
	query_str = 'something special'

	def setUp(self):
		self.C = searcher()
		self.C.count(testAll.eta)
		self.C.initTables()

	def tearDown(self):
		self.C.wordsdb.execute('drop table if exists urllist')
		self.C.wordsdb.execute('drop table if exists wordlist')
		self.C.wordsdb.execute('drop table if exists wordgraph')
		self.C.wordsdb.execute('drop index if exists urlidx')
		self.C.wordsdb.execute('drop index if exists wordidx')
		self.C.wordsdb.execute('drop index if exists graphidx')

	# def test_build(self):
	# 	pass
	
	def test_simpleQuery(self):
		self.C.buildIndex()
		print "searching for '"+testAll.query_str+"'"
		(tokens, results) = self.C.simpleQuery(testAll.query_str)
		for r in results:
			print r

if __name__ == '__main__':
	unittest.main()

	